data:
  data_percentage: 0.3
  input_file: telugu_results.json
  max_length: 2048
  min_length: 50
  output_dir: processed_data
  test_ratio: 0.05
  train_ratio: 0.9
  val_ratio: 0.05
integration:
  hf_token: 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
  hub_model_id: null
  push_to_hub: true
  seed: 42
  wandb_api_key: bc746fafa585f61730cdfd3c8226492c777f875a
  wandb_entity: null
  wandb_project: telugu-gemma-sft
logging:
  eval_steps: 250
  logging_steps: 50
  output_dir: finetuned_model
  report_to:
  - wandb
  - tensorboard
  save_steps: 500
  save_total_limit: 5
model:
  chat_template: '{% for message in messages %}

    {% if message[''role''] == ''user'' %}

    <start_of_turn>user

    {{ message[''content''] }}

    <end_of_turn>

    {% elif message[''role''] == ''assistant'' %}

    <start_of_turn>model

    {{ message[''content''] }}

    <end_of_turn>

    {% endif %}

    {% endfor %}

    {% if add_generation_prompt %}

    <start_of_turn>model

    {% endif %}'
  model_name: google/gemma-3-12b-pt
  use_chat_template: true
training:
  bf16: true
  bnb_4bit_compute_dtype: bfloat16
  deepspeed: null
  early_stopping_patience: 3
  fp16: false
  gradient_accumulation_steps: 32
  gradient_checkpointing: true
  learning_rate: 2.0e-06
  lora_alpha: 32
  lora_dropout: 0.05
  lora_r: 16
  lr_scheduler_type: cosine
  max_grad_norm: 1.0
  max_seq_length: 2048
  num_train_epochs: 3
  per_device_eval_batch_size: 2
  per_device_train_batch_size: 2
  use_4bit: false
  use_peft: false
  warmup_ratio: 0.05
  weight_decay: 0.01
